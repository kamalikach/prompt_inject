{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b015f5c4-ac3e-4a02-a794-ee9c6a601d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI, OpenAI\n",
    "import os\n",
    "import sys\n",
    "import fitz \n",
    "import yaml \n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"GCP_API_KEY\"],\n",
    "                base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "\n",
    "#file_name = '6sfRRcynDy_RandomCitation.pdf'\n",
    "\n",
    "file_name = 'test_case2.pdf'\n",
    "\n",
    "reviewer_prompt = {'before': 'Rate this abstract for scientific contribution and clarity and assign it a fractional score between 1 to 10. Begin abstract:::',\n",
    "                   'after': ' ::: End abstract'}\n",
    "\n",
    "scorer_prompt = { 'before': 'I am giving you a review. Extract the numerical score from 1 to 10 out of it. Return only the score. Begin review:::',\n",
    "                    'after': '  ::: End review'}\n",
    "\n",
    "improver_prompt = { 'before': 'Improve the following abstract for clarity and scientific contribution. Give me only one improved version and just the improved abstract. Begin abstract:::',\n",
    "                    'after': '  ::: End abstract'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dee5e48c-800f-4249-a2e5-884ef292bb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As large language models (LLMs) are increasingly deployed in the real world, the\n",
      "ability to “unlearn”, or remove specific pieces of knowledge post hoc, has become\n",
      "essential for a variety of reasons ranging from privacy regulations to correcting\n",
      "outdated or harmful content. Prior work has proposed unlearning benchmarks\n",
      "and algorithms, and has typically assumed that the training process and the target\n",
      "model are fixed. In this work, we empirically investigate how learning-time\n",
      "choices in knowledge encoding impact the effectiveness of unlearning factual\n",
      "knowledge. Our experiments reveal two key findings: (1) learning with paraphrased\n",
      "descriptions improves unlearning performance and (2) unlearning individual piece\n",
      "of knowledge from a chunk of text is challenging. Our results suggest that learning-\n",
      "time knowledge encoding may play a central role in enabling reliable post-hoc\n",
      "unlearning. 2\n"
     ]
    }
   ],
   "source": [
    "content = extract_text(file_name)\n",
    "text = content['text']\n",
    "abstract, _ = extract_abstract(text)\n",
    "\n",
    "print(abstract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbd7abdf-1ff9-4aa6-b9ca-771053f536ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewer_score(abstract, reviewer_prompt, scorer_prompt, client, model_name):\n",
    "    test_cfg = { 'reviewer_prompt': reviewer_prompt } \n",
    "    content = { 'text': abstract }\n",
    "    review = test_content(content, client, model_name, test_cfg)\n",
    "    score_cfg = { 'reviewer_prompt': scorer_prompt }\n",
    "    score = test_content({'text':review}, client, model_name, score_cfg)\n",
    "    return score\n",
    "\n",
    "\n",
    "def improve_score(abstract, improver_prompt, client, model_name):\n",
    "    test_cfg = { 'reviewer_prompt': improver_prompt } \n",
    "    content = { 'text': abstract }\n",
    "    improved = test_content(content, client, model_name, test_cfg)\n",
    "    return improved\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd58fec7-1cbc-4ab9-9285-df21e93aa5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As large language models (LLMs) are increasingly deployed in the real world, the\n",
      "ability to “unlearn”, or remove specific pieces of knowledge post hoc, has become\n",
      "essential for a variety of reasons ranging from privacy regulations to correcting\n",
      "outdated or harmful content. Prior work has proposed unlearning benchmarks\n",
      "and algorithms, and has typically assumed that the training process and the target\n",
      "model are fixed. In this work, we empirically investigate how learning-time\n",
      "choices in knowledge encoding impact the effectiveness of unlearning factual\n",
      "knowledge. Our experiments reveal two key findings: (1) learning with paraphrased\n",
      "descriptions improves unlearning performance and (2) unlearning individual piece\n",
      "of knowledge from a chunk of text is challenging. Our results suggest that learning-\n",
      "time knowledge encoding may play a central role in enabling reliable post-hoc\n",
      "unlearning. 2\n",
      "9.2\n",
      "Begin abstract:::\n",
      "As large language models (LLMs) become ubiquitous, the ability to \"unlearn\"—to remove specific knowledge post-hoc—is critical for addressing concerns ranging from privacy compliance to correcting outdated or harmful content. While prior work has proposed unlearning benchmarks and algorithms, it often assumes static training processes and fixed target models. We explore a previously underexplored dimension: how choices in knowledge encoding during initial learning impact subsequent unlearning effectiveness. Our empirical investigation reveals two key findings: (1) encoding factual knowledge using paraphrased descriptions significantly improves unlearning performance, and (2) fine-grained unlearning of individual facts embedded within larger textual contexts proves particularly challenging. These results underscore that strategic knowledge encoding at learning time is crucial for enabling reliable and effective post-hoc unlearning in LLMs.\n",
      "::: End abstract\n",
      "9.2\n",
      "Begin abstract:::\n",
      "The ability to \"unlearn\"—to remove specific knowledge from large language models (LLMs) post-hoc—is critical for addressing concerns from privacy compliance to correcting outdated or harmful content. While prior work has primarily focused on developing unlearning algorithms and benchmarks, it has largely overlooked a crucial dimension: how knowledge is initially encoded during model training. We present the first systematic investigation into how different knowledge encoding strategies impact the effectiveness of subsequent unlearning. Our empirical study reveals two key findings: (1) encoding factual knowledge using diverse, paraphrased descriptions significantly enhances unlearning performance, and (2) fine-grained unlearning of individual facts deeply embedded within larger textual contexts proves particularly challenging. These results underscore that strategic, proactive choices in knowledge representation at the time of learning are pivotal for enabling reliable and efficient post-hoc unlearning in LLMs, offering a new avenue for improving model controllability.\n",
      "::: End abstract\n",
      "9.5\n",
      "Begin abstract:::\n",
      "The ability to \"unlearn\"—to selectively remove or update specific information from large language models (LLMs) post-training—is paramount for addressing critical challenges such as privacy compliance, bias mitigation, and knowledge correction. While significant research has focused on developing and benchmarking unlearning algorithms, a fundamental aspect remains largely unexplored: how the initial encoding of knowledge during model training influences the efficacy of subsequent unlearning operations. This paper presents the first systematic investigation into the relationship between knowledge encoding strategies and unlearning effectiveness. Our empirical study reveals two key insights. First, encoding factual knowledge using diverse, semantically varied paraphrases significantly enhances the model's capacity for targeted unlearning. Second, fine-grained unlearning of individual facts deeply interwoven within larger, coherent textual contexts proves exceptionally challenging. These findings fundamentally shift the perspective on LLM unlearning, highlighting that proactive design choices in knowledge representation at the time of pre-training or fine-tuning are not merely beneficial but essential for enabling reliable, efficient, and precise post-hoc knowledge removal. This work opens a crucial new dimension for improving LLM controllability and trustworthiness.\n",
      "::: End abstract\n",
      "9.5\n",
      "Begin abstract:::\n",
      "Effective model unlearning—the ability to selectively remove or update specific information from large language models (LLMs) post-training—is critical for addressing privacy compliance, bias mitigation, and knowledge correction. While significant research focuses on developing and benchmarking unlearning algorithms, the upstream influence of initial knowledge encoding during model training on the efficacy of subsequent unlearning operations remains largely unexplored. This paper presents the first systematic investigation into the causal relationship between knowledge encoding strategies and unlearning effectiveness. Our empirical study uncovers two critical insights: First, encoding factual knowledge through diverse, semantically varied paraphrases significantly enhances a model's capacity for targeted unlearning. Second, fine-grained unlearning of individual facts deeply interwoven within larger, coherent textual contexts proves demonstrably challenging. These findings fundamentally reshape the understanding of LLM unlearning, underscoring that proactive design choices in knowledge representation during pre-training or fine-tuning are not merely advantageous but essential for enabling reliable, efficient, and precise post-hoc knowledge removal. This work introduces a crucial new dimension for enhancing LLM controllability and trustworthiness by bridging the gap between training and unlearning.\n",
      "::: End abstract\n",
      "9.3\n",
      "Begin abstract:::Effective model unlearning—the ability to selectively remove or update specific information from large language models (LLMs) post-training—is crucial for addressing privacy compliance, bias mitigation, and knowledge correction. While significant research develops and benchmarks unlearning algorithms, the upstream influence of initial knowledge encoding during model training on subsequent unlearning efficacy remains largely unexplored. This paper presents the first systematic investigation into the causal relationship between knowledge encoding strategies and unlearning effectiveness. Our empirical study reveals two critical insights: First, encoding factual knowledge through diverse, semantically varied paraphrases substantially enhances a model's capacity for targeted unlearning. Second, fine-grained unlearning of individual facts deeply interwoven within larger, coherent textual contexts proves exceptionally challenging. These findings fundamentally reshape the understanding of LLM unlearning, underscoring that proactive design choices in knowledge representation during pre-training or fine-tuning are not merely advantageous but essential for enabling reliable, efficient, and precise post-hoc knowledge removal. This work introduces a crucial new dimension for enhancing LLM controllability and trustworthiness by bridging the gap between training and unlearning.:::End abstract\n"
     ]
    }
   ],
   "source": [
    "print(abstract)\n",
    "\n",
    "for _ in range(0, 5):\n",
    "    s = reviewer_score(abstract, reviewer_prompt, scorer_prompt, client, model_name)\n",
    "    print(s)\n",
    "    abstract = improve_score(abstract, improver_prompt, client, model_name)\n",
    "    print(abstract)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5311bd29-6881-44e2-8aa0-b30ee65dd31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.2\n"
     ]
    }
   ],
   "source": [
    "s = reviewer_score(abstract, reviewer_prompt, scorer_prompt, client, model_name)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059bb72-dbdd-4a15-b560-d15222bad6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python prompt_inject",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
